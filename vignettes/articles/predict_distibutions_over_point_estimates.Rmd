---
title: "Predict Distibutions over Point Estimate"
bibliography: [../../inst/REFERENCES.bib]
biblio-style: apalike
link-citations: yes
editor_options: 
  markdown: 
    wrap: 80
---

<!-- Compress random variables into distribution objects -->

```{r, include = FALSE}
source(file.path(usethis::proj_get(), "vignettes",  "_common.R"))
```

## Motivation

<!-- Problem statement -->

Prediction is an error-prone activity. Depending on the business context, it may
be desirable to quantify the uncertainty using prediction intervals. Or,
different error types, i.e. under/over prediction, may be associated with
different costs. Thus, in some cases, biasing the model towards conservative
prediction might be translated into avoidance/favouring of one error type over
another.

At the stage of making predictions, typically and most often by default,
predictions are represented by a point estimation, such as the mean or median.
This is because representing each prediction as one number is convenient, even
though there is more information available at the stage of making the
prediction.

The problem is that representing prediction by merely a point estimation does
not allow us to model the uncertainty of that prediction. Thus, if one needs to
consider the uncertainty of the estimated value, e.g. for building prediction
intervals, one has to revamp the data structure and codebase of the application
to accommodate the new requirements. Doing so might entail (potentially
prohibitive) effort spent refactoring and communicating the changes with
other programmers. Therefore, new features that incorporate prediction
uncertainty might either get skipped or delayed until the changes are ready.


## Typical Approaches for Representing Random Variables

The ability to make correct inference on a random variable is influence by the
variable size. As the variable size gets bigger, our ability to make actionable
inference increases. For example, measuring people hight and taking the 95 hight
quantile allows automotive designers to decide on the space required between the
car seat and its ceiling.

Increasing random variable sizes requires us to devise a strategy for how to
represent them for further analysis, especially, in the case of multiple random
variable. For example, say we investigate the relationship of cars'
specifications and their fuel consumption measured in miles per gallon [MPG].
Here we focus on the $1^{st}$ and $3^{rd}$ cars in the 'mtcars' dataset.

```{r, echo=TRUE}
cars <- mtcars[c(1,3), ] %>% tibble::rownames_to_column("model")
print(cars)
```

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("https://imgur.com/nEnp4Os.png")
```

We start by taking *Mazda RX4* for a ride, and measuring how many miles it
covers before it consumes one gallon. We drive until the car consumes its 17 gallons fuel tank. The MPG measurement is:

```{r, echo=TRUE}
Mazda_RX4 = c(22, 24, 18, 22, 18, 29, 26, 25, 23, 21, 22, 22, 17, 21, 19, 20, 24)
```

We do the same for the *Datsun 710* 12 gallons fuel tank:

```{r, echo=TRUE}
Datsun_710 = c(29, 27, 24, 26, 24, 23, 21, 25, 17, 18, 22, 25)
```

The question then is how to represent the two cars MPG in adjacent to their
specifications:

```{r, echo=TRUE}
cars_specs <- cars %>% dplyr::select(-mpg) 
print(cars_specs)
```

There are two common approaches:

1. Convert each random vector into a point estimate with an estimator such
as the median:

```{r, echo = TRUE}
cars <- data.frame(mpg = c(median(Mazda_RX4), median(Datsun_710)), cars_specs)
print(cars)
```

2. Add an `mpg` column, where each cell is a list containing all the information
available for a particular car:

```{r, echo = TRUE}
cars <- data.frame(mpg = I(list(Mazda_RX4, Datsun_710)), cars_specs)
print(cars)
```

While representing both the MPG measurements and the cars' specification in a
table is convenient for further analysis, there are some drawbacks to consider:

1. The first approach, which is also the most common one, exhibits information
loss when the vector is compressed into a scalar. As a result of the information
loss, there is no way to calculate other important statistics such as the
interquartile range (IQR) or mean.

2. The second approach can be memory/computational prohibitive, especially as
the number of samples in the random vector increases.

This article suggests a third approach that keeps important information and
bounds the size of information representation, regardless of the sample size.
This approach is based on statistical distributions.

## Working with Distribution Objects

### What is a distribution object?

A **distribution object** preserves important statistical qualities of a random
variable in a succinct manner. Using the
[`distributional`](https://cran.r-project.org/web/packages/distributional/index.html)
R package, we can compress the following vector

```{r, echo=TRUE}
mpg_vec <- c(29, 27, 24, 26, 24, 23, 21, 25, 17, 18, 22, 25)
print(mpg_vec)
```

into a **distribution object** of length 1

```{r, echo=TRUE}
mpg_dist <- distributional::dist_normal(mu = mean(mpg_vec), sigma = sd(mpg_vec))
print(mpg_dist)
```

```{r, warning=TRUE}
warning("By using the normal distribution, a random variable of **any length** can be summarised by **two parameters**, the sample mean and standard deviation.")
```


A **distribution object** allows us to move a from `data.frame` with point
estimate column into a `data.frame` with a distribution column. For example,
instead of compressing the 'mpg' measurements of each car in the 'mtcars'
dataset into point estimations (the sample mean)

```{r, echo=FALSE}
data(mtcars, package = "datasets")
(
  mtcars
  %>% tibble::rownames_to_column("model")
  %>% tibble::as_tibble() 
  %>% dplyr::slice(c(1,3)) 
)
```

We can capture more information about the *mpg* measurements by using
**distribution objects**

```{r, echo=FALSE}
set.seed(1104)
(
  mtcars
  %>% tibble::rownames_to_column("model")
  %>% dplyr::rowwise() 
  %>% dplyr::mutate(mpg = distributional::dist_normal(mpg, round(runif(1, 1,3),1)))
  %>% dplyr::ungroup()
  %>% dplyr::slice(c(1,3))
)
```


### What operations can we perform on a distribution object?

Firstly, you can easily move from a **distribution object** into a point estimate by
using `distributional` functions:

```{r, echo=TRUE}
# MPG measurements in a vector 
mpg_vec <- c(29, 27, 24, 26, 24, 23, 21, 25, 17, 18, 22, 25)
# MPG measurements in a distribution object
mpg_dist <- distributional::dist_normal(mu = mean(mpg_vec), sigma = sd(mpg_vec))

# Comparison of vector-wise and distribution-wise operations
mean(mpg_vec); mean(mpg_dist)
median(mpg_vec); median(mpg_dist)
var(mpg_vec); distributional::variance(mpg_dist)

quantile(mpg_vec, p=0.75)[[1]]; quantile(mpg_dist, p=0.75)
min(mpg_vec); quantile(mpg_dist, p=0.01) # min approximation
max(mpg_vec); quantile(mpg_dist, p=0.99) # max approximation
```

Second, you can easily move from a **distribution object** into *Confidence Intervals* by using `distributional::hilo`:

```{r, echo=TRUE}
# z-confidence interval
c(
  mean(mpg_vec) - 1.96 * sd(mpg_vec)/sqrt(length(mpg_vec)),
  mean(mpg_vec) + 1.96 * sd(mpg_vec)/sqrt(length(mpg_vec))
) %>% round()

# quantile confidence interval
quantile(mpg_vec, probs = c(0.025, 0.975)) %>% round()

# distributional employs quantile confidence interval rather than z-confidence interval
distributional::hilo(mpg_dist, size = 95) %>% round()
```

```{r, warning=TRUE}
warning("Use `distributional::new_hilo` if you need a different type of confidence interval")
```

```{r, echo=TRUE}
# instantiate a z-confidence interval of type `hilo`
z_ci <- distributional::new_hilo(
  lower = mean(mpg_vec) - 1.96 * sd(mpg_vec)/sqrt(length(mpg_vec)),
  upper = mean(mpg_vec) + 1.96 * sd(mpg_vec)/sqrt(length(mpg_vec)),
  size = 95
)

print(z_ci)
```

Finally, you can work with **distribution objects** stored in a data.frame as
you would do with any other atomic type:

```{r, echo=TRUE}
# Generate a data.frame with distribution objects
cars <- data.frame(
  model = c("Mazda RX4", "Datsun 710"),
  mpg = c(distributional::dist_normal(21, 2), distributional::dist_normal(22.8, 3))
)
print(cars)

# Extracting 'mpg' mean point estimation
cars %>% dplyr::mutate(mpg = mean(mpg))

# Extracting 'mpg' 95% confidence interval
cars %>% 
  dplyr::mutate(CI = distributional::hilo(mpg, 95)) %>% 
  dplyr::mutate(CI_LB = round(CI$lower), CI_UB = round(CI$upper)) %>% 
  dplyr::select(-CI)
```

```{r, message=TRUE}
message("See all available operations on the [`distributional` package website](https://pkg.mitchelloharawild.com/distributional/).")
```


## Generating Distribution Objects

### Generating a distribution from point estimation

Compressing a random variable into a point estimation entails information loss.
It is not possible to restore the original distribution of a random variable
without prior information. Yet `distributional::dist_degenerate` allows us to
cast the point estimation into a **distribution object**.

For example, representing the 'mpg' column within the 'mtcars' dataset can be
achieved by a combination of `dplyr::mutate` and
`distributional::dist_degenerate`:
```{r, echo=TRUE}
mtcars %>% 
  tibble::as_tibble(rownames = "model") %>% 
  dplyr::mutate(mpg = distributional::dist_degenerate(mpg)) %>% 
  dplyr::filter(model %in% c("Mazda RX4", "Datsun 710"))
```

### Generating a distribution from bootstrap sampling

Steps:

1. Define functions for (a) model fitting, and (b) model prediction;
2. Perform bootstrap sampling on the data;
3. Compress the data with **distribution objects**; and
4. Bind the **distribution objects** with the rest of the data.

```{r, echo=TRUE, cache=TRUE}
# Step 1: define functions for (a) model fitting, and (b) model prediction 
fit_model <- function(split)
  lm(formula = mpg ~ ., data = rsample::analysis(split)) 

predict_model <- function(split, mdl)
  predict(object = mdl, newdata = rsample::assessment(split))

# Step 2: Perform bootstrap sampling on the data
set.seed(1623)

sampler <- rsample::bootstraps(mtcars, times = 200)

sampler$mdl <- purrr::map(sampler$splits, fit_model)

sampler$fit <- purrr::map2(sampler$splits, sampler$mdl, predict_model)

# Step 3: Compress the data with distribution objects
(
  mpg_distribution <- sampler$fit
  %>% dplyr::bind_rows()
  %>% tidyr::pivot_longer(dplyr::everything())  
  %>% dplyr::mutate(value = ifelse(value <= 0, NA_real_, value)) # MPG is strictly positive
  %>% tidyr::drop_na(value) # drop former non-positive MPG values
  %>% dplyr::rename(model = name) 
  %>% dplyr::group_by(model) # calculate MPG mean and standard deviation for each car 
  %>% dplyr::summarise(mu = mean(value), sigma = sd(value))
  %>% dplyr::mutate(mpg = distributional::dist_normal(mu = mu, sigma = sigma))
  %>% dplyr::select(-mu, -sigma)
  %>% dplyr::filter(model %in% c("Mazda RX4", "Datsun 710"))
)

# Step 4: Bind MPG distribution objects with cars specification
(
  tibble::as_tibble(mtcars, rownames = "model") %>% dplyr::select(-mpg)
  %>% dplyr::left_join(mpg_distribution) 
  %>% dplyr::select(model, mpg, dplyr::everything()) 
)
```

Alternatively, we can fit a [Gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution) to the data

```{r, echo=TRUE, cache=TRUE}
(
  mpg_distribution <- sampler$fit
  %>% dplyr::bind_rows()
  %>% tidyr::pivot_longer(dplyr::everything())
  %>% dplyr::mutate(value = ifelse(value <= 0, NA_real_, value))
  %>% tidyr::drop_na(value)
  %>% dplyr::rename(model = name)
  %>% dplyr::group_by(model)
  %>% dplyr::summarise(
    estimate = list(MASS::fitdistr(value, densfun = "gamma")$estimate),
    mpg = distributional::dist_gamma(
      shape = estimate[[1]][1], 
      rate = estimate[[1]][2]
    )
  )
)

# Bind mpg distribution objects with cars specification
(
  tibble::as_tibble(mtcars, rownames = "model") %>% dplyr::select(-mpg)
  %>% dplyr::left_join(mpg_distribution) 
  %>% dplyr::select(model, mpg, dplyr::everything()) 
  %>% dplyr::filter(model %in% c("Mazda RX4", "Datsun 710"))
)
```

### Generating a distribution from point estimation and standard deviation

Some learning algorithms, such as `lm`, can provide both the standard deviation
and the mean of a prediction. Using this information alleviates the generation
of **distribution objects**:

```{r, echo=TRUE}
# Split the data into train and test sets. 
train_set <- mtcars[c(-1, -3),]
test_set <- mtcars[c(1, 3),] # 1 = Mazda RX4, 3 = Datsun 710

# Fit a linear regression model on the train set
mdl <- lm(formula = mpg ~ ., data = train_set)
```

Reading `predict.lm` help file, we see an option to report the standard error of predictions

```{r, echo = FALSE, out.width = "70%"}
knitr::include_graphics("https://i.imgur.com/1bcFZSG.png")
```

We set `se.fit = TRUE` and predict the test set

```{r, echo=TRUE}
# Predict the test set
y_hat <- predict.lm(object = mdl, newdata = test_set, se.fit = TRUE)
y_hat
```

Finally, we instantiate normal **distribution objects** and put them inside a `data.frame` using `enframe`:

```{r, echo=TRUE}
# Create a data.frame with the prediction distribution using the prediction
# point estimate (fit) and its associated standard deviation (se.fit)
(
    distributional::dist_normal(mu = y_hat$fit, sigma = y_hat$se.fit) 
    %>% tibble::enframe(name = "model", value = "mpg")
    %>% dplyr::bind_cols(test_set %>% dplyr::select(-mpg))
)
```      

## Conclusion

Prediction is an error-prone activity. Unless there is a business requirement
that prohibits you from storing each prediction's distribution, storing merely
point estimations is a premature decision.

Instead, using **distribution objects** makes your code structure independent of
business requirements that involve the consideration of prediction uncertainty.
With **distribution objects**, you can (a) defer the decision about which point
estimator to use; and (b) provide additional statical estimators such as
prediction intervals.

